{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "479830b0",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "# Categorización de publicaciones de productos de Mercado Libre\n",
    "\n",
    "Autores: Maximiliano Tejerina, Eduardo Barseghian, Benjamín Ocampo\n",
    "\n",
    "En `items_exploration.ipynb` se trabajó sobre un conjunto reducido de títulos\n",
    "etiquetados de publicaciones de Mercado Libre, donde se analizó la\n",
    "confiabilidad de las etiquetas, frecuencia de palabras, y dependencia de\n",
    "variables, con el objetivo de obtener información que influya en la predicción\n",
    "de la categoría más adecuada de un producto dado el título de su publicación.\n",
    "Con el fin de utilizar modelos de *machine learning* en esta tarea, es\n",
    "necesario realizar una codificación de los datos tabulados. Por ende, esta\n",
    "notebook analiza los distintos pasos necesarios para obtener una matriz\n",
    "resultante que pueda ser evaluada en modelos de clasificación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7eb327",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "## Definición de funciones y constantes *helper*\n",
    "\n",
    "Al igual que en la exploración, se definen algunas funcionalidades que serán de\n",
    "utilidad durante la codificación, pero que no son muy relevantes para el\n",
    "seguimiento de este trabajo. No obstante, con el fin de mantener la\n",
    "reproducibilidad con herramientas *online* tales como `Google Colab`, son\n",
    "mantenidas en una sección dentro de esta notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26575d90",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import (text, sequence)\n",
    "from nltk import (download, corpus, tokenize)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from unidecode import unidecode\n",
    "\n",
    "download(\"stopwords\")\n",
    "download('punkt')\n",
    "\n",
    "stopwords = (set(corpus.stopwords.words(\"spanish\")) |\n",
    "             set(corpus.stopwords.words(\"portuguese\")))\n",
    "\n",
    "URL = \"https://www.famaf.unc.edu.ar/~nocampo043/ml_challenge2019_dataset.csv\"\n",
    "df = pd.read_csv(URL)\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Given a string @s the following parsing is performed:\n",
    "        - Removes non-ascii characters.\n",
    "        - Removes numbers and special symbols.\n",
    "        - Expand common contractions.\n",
    "        - Removes stopwords.\n",
    "    \"\"\"\n",
    "    numbers = r\"\\d+\"\n",
    "    symbols = r\"[^a-zA-Z ]\"\n",
    "    # TODO: See if we can find more contractions.\n",
    "    contractions = [\n",
    "        (\"c/u\", \"cada uno\"),\n",
    "        (\"c/\", \"con\"),\n",
    "        (\"p/\", \"para\"),\n",
    "    ]\n",
    "    # TODO: Can we avoid chaining assignments? Maybe function composition.\n",
    "    s = unidecode(s)\n",
    "    s = s.lower()\n",
    "    for expression, replacement in contractions:\n",
    "        s = s.replace(expression, replacement)\n",
    "    s = re.sub(numbers, \"\", s)\n",
    "    s = re.sub(symbols, \"\", s)\n",
    "    s = ' '.join(w for w in tokenize.word_tokenize(s) if not w in stopwords)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91de5f19",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "## Limpieza de Texto\n",
    "\n",
    "Si se observan algunos ejemplos de títulos en el conjunto de datos se puede ver\n",
    "que varios de ellos utilizan números, símbolos especiales, o combinaciones de\n",
    "mayúsculas y minúsculas con el fin de capturar la atención del lector. Sin\n",
    "embargo, una palabra que es escrita de distintas maneras no es relevante para\n",
    "catalogar dichas publicaciones. De manera similar ocurre con caracteres\n",
    "especiales, números, e incluso artículos o proposiciones. Por ende, se procedió\n",
    "a realizar un preprocesamiento de cada uno de los títulos donde se reemplazan:\n",
    "\n",
    " - Mayúsculas por minúsculas.\n",
    " - Caracteres que no tienen una codificación ascii.\n",
    " - Números y símbolos.\n",
    " - Contracciones de palabras por su expresión completa.\n",
    " - *Stopwords*\n",
    "\n",
    "Esto es realizado por la función `clean_text` siendo aplicada para cada uno de\n",
    "los títulos del conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e323084c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "df[\"title\"].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ee5494",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "df = df.assign(cleaned_title=df[\"title\"].apply(clean_text))\n",
    "df[[\"title\", \"cleaned_title\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85cbcb2",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "## Codificación de títulos: tokenización y secuencias\n",
    "\n",
    "Una vez realizada la limpieza de los títulos, se procedió a identificar cada una\n",
    "de las palabras que ocurren en ellos, confeccionando de esta manera un\n",
    "vocabulario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f69e0e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "word_tokenizer = text.Tokenizer()\n",
    "word_tokenizer.fit_on_texts(df[\"cleaned_title\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55eae19",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "Esta identificación de palabras o *tokens* es realizada por una instancia de la\n",
    "clase `Tokenizer` de `Keras` a partir de `fit_on_texts` dando lugar a algunas\n",
    "propiedades y métodos, como la cantidad de oraciones utilizadas, frecuencia de\n",
    "palabras, y orden por tokens más frecuentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce8552a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "word_tokenizer.word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540c4904",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "word_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2fbb85",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "En este caso, el atributo `word_index` muestra el vocabulario obtenido, donde a\n",
    "cada palabra se le asigna un único índice según su frecuencia dada por\n",
    "`word_counts`. Por ejemplo, el *token* `maquina` tiene mayor frecuencia que\n",
    "todas las palabras en el vocabulario y por ende se le asigna el índice 1, luego\n",
    "les siguen `x` con el índice 2, y así sucesivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7d3ec9",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "encoded_titles = word_tokenizer.texts_to_sequences(df[\"cleaned_title\"])\n",
    "encoded_titles[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4d3f7f",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "Basándose en esto, se codificaron los títulos asignándole un vector\n",
    "correspondiente a los índices de cada una de las palabras que lo componen por\n",
    "medio de `texts_to_sequences`. Por ejemplo, el título `galoneira semi\n",
    "industrial` se le asigna el vector `[576, 186, 40]`, ya que `galoneira`, `semi`,\n",
    "e `industrial` son las palabras 576, 186, y 40 más frecuente del vocabulario, es\n",
    "decir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb95f710",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "(\n",
    "    word_tokenizer.word_index[\"galoneira\"],\n",
    "    word_tokenizer.word_index[\"semi\"],\n",
    "    word_tokenizer.word_index[\"industrial\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a47dac",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "Ahora bien, los vectores obtenidos tienen largos distintos según la cantidad de\n",
    "palabras que un título presente. Dado que un modelo requiere datos de entrada de\n",
    "una dimensión concreta, se rellenó con 0’s los vectores de representación de las\n",
    "palabras hasta obtener una longitud en común."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c1f43b",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "encoded_titles = sequence.pad_sequences(encoded_titles, padding='post')\n",
    "encoded_titles[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f86d42",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "encoded_titles.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b6a1f9",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "## Codificación de etiquetas: *Label encoding*\n",
    "\n",
    "De igual manera que para los títulos, se necesitó codificar sus categorías\n",
    "asignadas. Sin embargo, no fue por medio de secuencias de números sino más bien\n",
    "a través de un índice por cada etiqueta, usando una instancia de la clase\n",
    "`LabelEncoder` de la librería `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b504b00",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "encoded_labels = le.fit_transform(df[\"category\"])\n",
    "encoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01521ae",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "le.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35dd767",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "`encoded_labels` corresponde a la misma columna de datos `category` del\n",
    "*dataframe* `df` con la diferencia de haber reemplazado cada etiqueta por un\n",
    "número entre 0 y el total de categorías. Luego, en caso de querer realizar el\n",
    "paso inverso de decodificación se puede usar el método `inverse_transorme` del\n",
    "codificador `le`. Por ejemplo, la etiqueta cuyo índice es 15 corresponde con\n",
    "`SEWING_MACHINES`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be46616d",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "le.inverse_transform([15])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae06083",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "## *Word Embeddings*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca22468",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "### Custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b8384c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc50acb",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "nof_out_of_vocab_rep = 1\n",
    "vocab_size = len(word_tokenizer.word_index) + nof_out_of_vocab_rep\n",
    "_, length_long_sentence = encoded_titles.shape\n",
    "output_dim = 64\n",
    "embedding = Embedding(vocab_size, output_dim, input_length=length_long_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34be906",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "### Pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542e4782",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_markers": "\"\"\"",
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
