{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "clean_dataset.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "lines_to_next_cell": 0,
        "id": "NGGm7T8hbZKW"
      },
      "source": [
        " # Categorización de publicaciones de productos de Mercado Libre\n",
        "\n",
        " Autores: Maximiliano Tejerina, Eduardo Barseghian, Benjamín Ocampo"
      ],
      "id": "NGGm7T8hbZKW"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "lines_to_next_cell": 0,
        "id": "Ulye41w3bZKc"
      },
      "source": [
        "## Funciones y Constantes"
      ],
      "id": "Ulye41w3bZKc"
    },
    {
      "cell_type": "code",
      "metadata": {
        "lines_to_next_cell": 0,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RirHIg6TbZKe",
        "outputId": "4a48bd3a-6a17-4ca3-f856-7b4538c31feb"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy\n",
        "import nltk\n",
        "!pip install unidecode\n",
        "from unidecode import unidecode\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn import preprocessing\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "id": "RirHIg6TbZKe",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.7/dist-packages (1.2.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "lines_to_next_cell": 0,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jc5dVhkqbZKh",
        "outputId": "26f4892c-4baa-4b5c-ce2e-76f8434cf0e6"
      },
      "source": [
        "nltk.download(\"stopwords\")\n",
        "nltk.download('punkt')\n",
        "\n",
        "stopwords = \\\n",
        "    set(nltk.corpus.stopwords.words(\"spanish\")) | \\\n",
        "    set(nltk.corpus.stopwords.words(\"portuguese\"))\n",
        "\n",
        "\n",
        "def remove_unimportant_words(s):\n",
        "    \"\"\"\n",
        "    Removes from the string @s all the stopwords, digits, and special chars\n",
        "    \"\"\"\n",
        "    special_chars = \"-.+,[@_!#$%^&*()<>?/\\|}{~:]\"\n",
        "    digits = \"0123456789\"\n",
        "    invalid_chars = special_chars + digits\n",
        "\n",
        "    reduced_title = ''.join(c for c in s if not c in invalid_chars)\n",
        "\n",
        "    reduced_title = ' '.join(w.lower() for w in word_tokenize(reduced_title)\n",
        "                             if not w.lower() in stopwords)\n",
        "    return reduced_title\n",
        "\n",
        "\n",
        "def expand_contractions(s): \n",
        "    title = s\n",
        "    contractions = {\" c/u \": \"cada uno\", \" p/\": \"para\", \" c/\": \"con\"}\n",
        "    for key, value in contractions.items():\n",
        "        title = title.split(key)\n",
        "        title = value.join(title)\n",
        "    return title\n",
        "\n",
        "def prepare_tokenizer(words):\n",
        "    \"\"\"\n",
        "    Function that generates a vocabulary, takes a list of words.\n",
        "    returns a list of tokenized words.    \n",
        "    \"\"\"\n",
        "    t = Tokenizer(filters='-.+,[@_!#$%^&*()<>?/\\|}{~:]0123456789', lower=True)\n",
        "    t.fit_on_texts(words)\n",
        "    encoded_docs = t.texts_to_sequences(words)\n",
        "    return pad_sequences(encoded_docs)"
      ],
      "id": "jc5dVhkqbZKh",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "lines_to_next_cell": 0,
        "id": "IoQE9CdIbZKj"
      },
      "source": [
        "URL = \"https://www.famaf.unc.edu.ar/~nocampo043/ml_challenge2019_dataset.csv\"\n",
        "df = pd.read_csv(URL)"
      ],
      "id": "IoQE9CdIbZKj",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgmSaloSbZKk"
      },
      "source": [
        "## Limpieza de Texto"
      ],
      "id": "cgmSaloSbZKk"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "lines_to_next_cell": 0,
        "id": "4aE-vX75bZKl"
      },
      "source": [
        "## Tokenización y Secuencias\n",
        "La función \"prepare_tokeneizer\", imita la función: \"remove_unimportant_words\". Permite vectorizar un corpus de texto, convirtiendo cada texto en una secuencia de números entero. \n",
        "Actualiza el vocabulario interno basado en una lista de textos. Esto es necesario  antes de usar texts_to_sequence. Cuya función transforma cada secuencia en una lista de texto. Solo se tendrán en cuenta las palabras conocidas por el tokenizador.\n",
        "\n",
        "## Tokenization and sequences\n",
        "The function \"prepare_tokeneizer\" looks like the function: \"remove_unimportant_words\". Allows you to vectorize a corpus of text, converting each text into a sequence of whole numbers.\n",
        "Updates internal vocabulary based on a list of texts. This is necessary before using \"texts_to_sequence\". Whose functionality is to transform each sequence into a text list. Only words known to the tokenizer will be taken into account."
      ],
      "id": "4aE-vX75bZKl"
    },
    {
      "cell_type": "code",
      "metadata": {
        "lines_to_next_cell": 0,
        "id": "xTNnOMZvbZKn"
      },
      "source": [
        "encoded_titles = prepare_tokenizer(df.title)"
      ],
      "id": "xTNnOMZvbZKn",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "lines_to_next_cell": 0,
        "id": "svPgBPw9bZKo"
      },
      "source": [
        "## Label Encoding"
      ],
      "id": "svPgBPw9bZKo"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBkq_j6G1HWx"
      },
      "source": [
        "EEncode target labels with value between 0 and n_classes-1.\n",
        "This transformer should be used to encode target values, i.e. y, and not the input X.\n",
        "\n"
      ],
      "id": "fBkq_j6G1HWx"
    },
    {
      "cell_type": "code",
      "metadata": {
        "lines_to_next_cell": 0,
        "id": "KBTGy6QObZKp"
      },
      "source": [
        "le = preprocessing.LabelEncoder()\n",
        "encoded_labels = le.fit_transform(df[\"category\"])"
      ],
      "id": "KBTGy6QObZKp",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "lines_to_next_cell": 0,
        "id": "e9Wv--_KbZKq"
      },
      "source": [
        "## Word Embeddings"
      ],
      "id": "e9Wv--_KbZKq"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxZ83ZVBbZKr"
      },
      "source": [
        ""
      ],
      "id": "QxZ83ZVBbZKr",
      "execution_count": null,
      "outputs": []
    }
  ]
}