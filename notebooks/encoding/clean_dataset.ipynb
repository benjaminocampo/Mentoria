{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "086d4869",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "# Categorización de publicaciones de productos de Mercado Libre\n",
    "\n",
    "Autores: Maximiliano Tejerina, Eduardo Barseghian, Benjamín Ocampo\n",
    "\n",
    "En `items_exploration.ipynb` se trabajó sobre un conjunto reducido de títulos\n",
    "etiquetados de publicaciones de Mercado Libre, donde se analizó la\n",
    "confiabilidad de las etiquetas, frecuencia de palabras, y dependencia de\n",
    "variables, con el objetivo de obtener información que influya en la predicción\n",
    "de la categoría más adecuada de un producto dado el título de su publicación.\n",
    "Con el fin de utilizar modelos de *machine learning* en esta tarea, es\n",
    "necesario realizar una codificación de los datos tabulados. Por ende, esta\n",
    "notebook analiza los distintos pasos necesarios para obtener una matriz\n",
    "resultante que pueda ser evaluada en modelos de clasificación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8496de34",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "## Definición de funciones y constantes *helper*\n",
    "\n",
    "Al igual que en la exploración, se definen algunas funcionalidades que serán de\n",
    "utilidad durante la codificación, pero que no son muy relevantes para el\n",
    "seguimiento de este trabajo. Con el fin de mantener la reproducibilidad con\n",
    "herramientas *online* tales como `Google Colab`, son mantenidas en una sección\n",
    "dentro de esta notebook. No obstante, en futuros trabajos se procederá a\n",
    "estructurar dichas funcionalidades en scripts separados manteniendo un único\n",
    "*entrypoint*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ff8643",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import io\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Flatten, Embedding, Dot\n",
    "from tensorflow.keras.preprocessing import text, sequence\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from unidecode import unidecode\n",
    "\n",
    "from nltk import (download, corpus, tokenize)\n",
    "download(\"stopwords\")\n",
    "download('punkt')\n",
    "\n",
    "stopwords = (set(corpus.stopwords.words(\"spanish\")) |\n",
    "             set(corpus.stopwords.words(\"portuguese\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8bb38e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class Word2Vec(Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_ns):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        self.target_embedding = Embedding(vocab_size,\n",
    "                                          embedding_dim,\n",
    "                                          input_length=1,\n",
    "                                          name=\"w2v_embedding\")\n",
    "        self.context_embedding = Embedding(vocab_size,\n",
    "                                           embedding_dim,\n",
    "                                           input_length=num_ns + 1)\n",
    "        self.dots = Dot(axes=(3, 2))\n",
    "        self.flatten = Flatten()\n",
    "\n",
    "    def call(self, pair):\n",
    "        target, context = pair\n",
    "        word_emb = self.target_embedding(target)\n",
    "        context_emb = self.context_embedding(context)\n",
    "        dots = self.dots([context_emb, word_emb])\n",
    "        return self.flatten(dots)\n",
    "\n",
    "\n",
    "def generate_skipgram_training_data(sequences, window_size, num_ns, vocab_size,\n",
    "                                    batch_size, buffer_size, seed):\n",
    "    \"\"\"\n",
    "    Generates skip-gram pairs with negative sampling for a list of sequences\n",
    "    (int-encoded sentences) based on window size, number of negative samples\n",
    "    and vocabulary size.\n",
    "    \"\"\"\n",
    "    # Elements of each training example are appended to these lists.\n",
    "    targets, contexts, labels = [], [], []\n",
    "\n",
    "    # Build the sampling table for vocab_size tokens.\n",
    "    sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(\n",
    "        vocab_size)\n",
    "\n",
    "    # Iterate over all sequences (sentences) in dataset.\n",
    "    for sequence in tqdm.tqdm(sequences):\n",
    "\n",
    "        # Generate positive skip-gram pairs for a sequence (sentence).\n",
    "        positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "            sequence,\n",
    "            vocabulary_size=vocab_size,\n",
    "            sampling_table=sampling_table,\n",
    "            window_size=window_size,\n",
    "            negative_samples=0)\n",
    "\n",
    "        # Iterate over each positive skip-gram pair to produce training examples\n",
    "        # with positive context word and negative samples.\n",
    "        for target_word, context_word in positive_skip_grams:\n",
    "            context_class = tf.expand_dims(\n",
    "                tf.constant([context_word], dtype=\"int64\"), 1)\n",
    "            negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "                true_classes=context_class,\n",
    "                num_true=1,\n",
    "                num_sampled=num_ns,\n",
    "                unique=True,\n",
    "                range_max=vocab_size,\n",
    "                seed=seed,\n",
    "                name=\"negative_sampling\")\n",
    "\n",
    "            # Build context and label vectors (for one target word)\n",
    "            negative_sampling_candidates = tf.expand_dims(\n",
    "                negative_sampling_candidates, 1)\n",
    "\n",
    "            context = tf.concat([context_class, negative_sampling_candidates],\n",
    "                                0)\n",
    "            label = tf.constant([1] + [0] * num_ns, dtype=\"int64\")\n",
    "\n",
    "            # Append each element from the training example to global lists.\n",
    "            targets.append(target_word)\n",
    "            contexts.append(context)\n",
    "            labels.append(label)\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
    "    dataset = dataset.shuffle(buffer_size).batch(batch_size,\n",
    "                                                 drop_remainder=True)\n",
    "    dataset = dataset.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def save_embedding(vocab, weights):\n",
    "    out_vectors = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
    "    out_metadata = io.open('metadata.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "    for index, word in enumerate(vocab):\n",
    "        if index != 0:\n",
    "            vec = weights[index]\n",
    "            out_vectors.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "            out_metadata.write(word + \"\\n\")\n",
    "    out_vectors.close()\n",
    "    out_metadata.close()\n",
    "\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Given a string @s the following parsing is performed:\n",
    "        - Lowercase letters.\n",
    "        - Removes non-ascii characters.\n",
    "        - Removes numbers and special symbols.\n",
    "        - Expand common contractions.\n",
    "        - Removes stopwords.\n",
    "    \"\"\"\n",
    "    numbers = r\"\\d+\"\n",
    "    symbols = r\"[^a-zA-Z ]\"\n",
    "    # TODO: See if we can find more contractions.\n",
    "    contractions = [\n",
    "        (\"c/u\", \"cada uno\"),\n",
    "        (\"c/\", \"con\"),\n",
    "        (\"p/\", \"para\"),\n",
    "    ]\n",
    "    # TODO: Can we avoid chaining assignments? Maybe function composition.\n",
    "    s = unidecode(s)\n",
    "    s = s.lower()\n",
    "    for expression, replacement in contractions:\n",
    "        s = s.replace(expression, replacement)\n",
    "    s = re.sub(numbers, \"\", s)\n",
    "    s = re.sub(symbols, \"\", s)\n",
    "    s = ' '.join(w for w in tokenize.word_tokenize(s) if not w in stopwords)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30c6d78",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "URL = \"https://www.famaf.unc.edu.ar/~nocampo043/ml_challenge2019_dataset.csv\"\n",
    "df = pd.read_csv(URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8795004b",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "## Limpieza de Texto\n",
    "\n",
    "Si se observan algunos ejemplos de títulos en el conjunto de datos se puede ver\n",
    "que varios de ellos utilizan números, símbolos especiales, o combinaciones de\n",
    "mayúsculas y minúsculas con el fin de capturar la atención del lector. Sin\n",
    "embargo, una palabra que es escrita de distintas maneras no es relevante para\n",
    "catalogar dichas publicaciones. De manera similar ocurre con caracteres\n",
    "especiales, números, e incluso artículos o proposiciones. Por ende, se procedió\n",
    "a realizar un preprocesamiento de cada uno de los títulos donde se reemplazan:\n",
    "\n",
    " - Mayúsculas por minúsculas.\n",
    " - Caracteres que no tienen una codificación ascii.\n",
    " - Números y símbolos.\n",
    " - Contracciones de palabras por su expresión completa.\n",
    " - *Stopwords*\n",
    "\n",
    "Esto es realizado por la función `clean_text` siendo aplicada para cada uno de\n",
    "los títulos del conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4632b8",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "df[\"title\"].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9445809",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "df = df.assign(cleaned_title=df[\"title\"].apply(clean_text))\n",
    "df[[\"title\", \"cleaned_title\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8158c2",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "## Codificación de títulos: tokenización y secuencias\n",
    "\n",
    "Una vez realizada la limpieza de los títulos, se procedió a identificar cada una\n",
    "de las palabras que ocurren en ellos, confeccionando de esta manera un\n",
    "vocabulario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624add1e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "word_tokenizer = text.Tokenizer()\n",
    "word_tokenizer.fit_on_texts(df[\"cleaned_title\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bf653c",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "Esta identificación de palabras o *tokens* es realizada por una instancia de la\n",
    "clase `Tokenizer` de `Keras` a partir de `fit_on_texts` dando lugar a algunas\n",
    "propiedades y métodos, como la cantidad de oraciones utilizadas, frecuencia de\n",
    "palabras, y orden por tokens más frecuentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3029b224",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "word_tokenizer.word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8531dc",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "word_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5243553a",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "En este caso, el atributo `word_index` muestra el vocabulario obtenido, donde a\n",
    "cada palabra se le asigna un único índice según su frecuencia dada por\n",
    "`word_counts`. Por ejemplo, el *token* `maquina` tiene mayor frecuencia que\n",
    "todas las palabras en el vocabulario y por ende se le asigna el índice 1, luego\n",
    "les siguen `x` con el índice 2, y así sucesivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601153e4",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "encoded_titles = word_tokenizer.texts_to_sequences(df[\"cleaned_title\"])\n",
    "encoded_titles[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4926e6b8",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "Basándose en esto, se codificaron los títulos asignándole un vector\n",
    "correspondiente a los índices de cada una de las palabras que lo componen por\n",
    "medio de `texts_to_sequences`. Por ejemplo, el título `galoneira semi\n",
    "industrial` se le asigna el vector `[576, 186, 40]`, ya que `galoneira`, `semi`,\n",
    "e `industrial` son las palabras 576, 186, y 40 más frecuente del vocabulario, es\n",
    "decir, dichos números son los índices asignados a cada una de esas palabras como\n",
    "puede a través de la siguiente celda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0511cf7f",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "(\n",
    "    word_tokenizer.word_index[\"galoneira\"],\n",
    "    word_tokenizer.word_index[\"semi\"],\n",
    "    word_tokenizer.word_index[\"industrial\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f61698",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "Ahora bien, los vectores obtenidos tienen largos distintos según la cantidad de\n",
    "palabras que un título presente. Dado que un modelo requiere datos de entrada de\n",
    "una dimensión concreta, se rellenó con 0’s los vectores de representación hasta\n",
    "obtener una longitud en común."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aada46fc",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "encoded_titles = sequence.pad_sequences(encoded_titles, padding='post')\n",
    "encoded_titles[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a177dde2",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "encoded_titles.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a1ddff",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "## Codificación de etiquetas: *Label encoding*\n",
    "\n",
    "De igual manera que para los títulos, se necesitó codificar sus categorías\n",
    "asignadas. Sin embargo, no fue por medio de secuencias de números sino más bien\n",
    "a través de un índice por cada etiqueta, usando una instancia de la clase\n",
    "`LabelEncoder` de la librería `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f8b3f6",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "encoded_labels = le.fit_transform(df[\"category\"])\n",
    "encoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6944f803",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "le.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0729083",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "`encoded_labels` corresponde a la misma columna de datos `category` del\n",
    "*dataframe* `df` con la diferencia de haber reemplazado cada etiqueta por un\n",
    "número entre 0 y el total de categorías. Luego, en caso de querer realizar el\n",
    "paso inverso de decodificación se puede usar el método `inverse_transorme` del\n",
    "codificador `le`. Por ejemplo, la etiqueta cuyo índice es 15 corresponde con\n",
    "`SEWING_MACHINES`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d34153",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "le.inverse_transform([15])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1e6564",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "## *Word Embeddings*\n",
    "\n",
    "En la sección anterior se representaron los títulos de las publicaciones por\n",
    "medio de vectores densos, donde una palabra era representada por el índice $i$,\n",
    "si esta era la $i-ésima$ palabra más frecuente. No obstante, también se pueden\n",
    "representar las palabras por medio de vectores que mantengan su semántica, es\n",
    "decir, aquellas que tengan un significado similar, tendrán por consecuente una\n",
    "representación vectorial similar.\n",
    "\n",
    "Un *word embedding* es una matriz de parámetros entrenables donde a cada palabra\n",
    "del vocabulario se le asigna un vector representante. En esta sección, se\n",
    "procedió a obtener esta matriz con 2 métodos distintos:\n",
    "\n",
    " - Entrenando los parámetros por medio de los datos disponibles (*Custom*).\n",
    " - Utilizando representaciones disponibles en la web (*Pretrained*)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95af8052",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "### *Custom*\n",
    "\n",
    "Para instanciar un *embedding* de las palabras del conjunto de datos, se\n",
    "necesitó obtener la longitud del título más largo, el tamaño del vocabulario, y\n",
    "la dimensión de los vectores representación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2b6959",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "_, long_sentence_size = encoded_titles.shape\n",
    "nof_dim_out_of_vocab = 1\n",
    "vocab_size = len(word_tokenizer.word_index) + nof_dim_out_of_vocab\n",
    "output_dim = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4cb559",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "Cabe recordar que el tamaño del vocabulario es la cantidad de palabras distintas\n",
    "que ocurren en los títulos de las publicaciones más el número de dimensiones\n",
    "para representar las palabras fuera del vocabulario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9966a9b5",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(vocab_size,\n",
    "                            output_dim,\n",
    "                            input_length=long_sentence_size)\n",
    "embedding_layer(tf.constant([1, 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fe28e8",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "Una vez instanciado el *embedding* se puede evaluarlo en índices que representan\n",
    "algúna palabra obteniendo la representación resultante. Por ejemplo, para los\n",
    "números 1 y 2 se obtuvieron los vectores dados por la celda anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc740b3",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "weights = embedding_layer.get_weights()[0]\n",
    "vocab = word_tokenizer.word_index.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cd78a2",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "Cabe recalcar nuevamente que los parámetros del *embedding* instanciado fueron\n",
    "asignados de manera aleatoria y por lo tanto las representaciones de las\n",
    "palabras no son acordes a su significado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c918c7",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "### *Word2Vec*\n",
    "\n",
    "Para entrenar los parámetros presentens en `weights` se utilizó el modelo\n",
    "`word2vec` que consiste en generar *skip-grams* a través de una lista de\n",
    "oraciones. Estos *skip-grams* son pares `(target, context)` donde dada una\n",
    "palabra `target`, se le asocia otra denominada `context` que puede o no\n",
    "encontrarse en su contexto. Si ocurre lo primero, se tiene un *skip-gram*\n",
    "positivo, y en caso contrario uno negativo. El objetivo fue generar para cada\n",
    "palabra en el vocabulario, un *skip-gram* positivo y una cantidad\n",
    "`nof_negative_skipgrams` negativos, y utilizarlos para entrenar un *embedding*\n",
    "personalizado. Varios aspectos de implementación fueron obtenidos desde las\n",
    "siguientes fuentes:\n",
    "\n",
    " - [Artículo ilustrativo de\n",
    "   word2vec](https://jalammar.github.io/illustrated-word2vec/)\n",
    " - [Tutorial de tensorflow y\n",
    "   keras](https://www.tensorflow.org/tutorials/text/word2vec#generate_skip-grams_from_one_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4587bd5",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "nof_negative_skipgrams = 4\n",
    "batch_size = 1024\n",
    "buffer_size = 10000\n",
    "dataset = generate_skipgram_training_data(\n",
    "    sequences=encoded_titles,\n",
    "    window_size=2,\n",
    "    num_ns=4,\n",
    "    vocab_size=vocab_size,\n",
    "    seed=seed,\n",
    "    batch_size=batch_size,\n",
    "    buffer_size=buffer_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04eabdb",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "Para la generación del conjunto de datos se utilizó la función\n",
    "`generate_skipgram_training_data` con una cantidad de 4 *skip-grams* negativos\n",
    "por 1 positivo y organizado en *mini-batches* para únicamente actualizar los\n",
    "parámetros tras haber evaluado un lote o *batch* y agilizar el entrenamiento.\n",
    "\n",
    "Debido al tamaño del conjunto de datos resultante, el entrenamiento puede llevar\n",
    "algunos minutos. Por ello, se dispuso la matriz de pesos obtenida bajo la\n",
    "semilla `seed` utilizada en un servidor para facilitar su acceso remóto.\n",
    "También, pueden ser visualizados por la herramienta [Embedding\n",
    "Projector](http://projector.tensorflow.org/). Para ello.\n",
    "\n",
    " - Seleccionar *Load data*.\n",
    " - Agregar los archivos\n",
    "   [metadata.tsv](https://drive.google.com/file/d/1QkmlfADvogUfWcH2iPG7oZnjivu6_Hdd/view?usp=sharing)\n",
    "   y\n",
    "   [vectors.tsv](https://drive.google.com/file/d/1XW5ndaAocWhUFXZPoSEGLuxGjYbmLydW/view?usp=sharing).\n",
    "\n",
    "En caso de no querer realizar el entrenamiento, se puede proceder a la siguiente\n",
    "sección donde se utilizó el embedding preentrenado y otros disponibles online."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a810b947",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "word2vec = Word2Vec(vocab_size, embedding_dim, nof_negative_skipgrams)\n",
    "word2vec.compile(optimizer='adam',\n",
    "                 loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                 metrics=['accuracy'])\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")\n",
    "word2vec.fit(dataset, epochs=20, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b4f337",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "weights = word2vec.get_layer('w2v_embedding').get_weights()[0]\n",
    "vocab = word_tokenizer.word_index.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4285189",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "save_embedding(vocab, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d845d4",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "Luego de la etapa de entrenamiento. Se utilizó la función `save_embedding` para\n",
    "almacenar en 2 archivos separados de manera estructurada para cada palabra en\n",
    "`vocab` su representación vectorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b4b976",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "### *Pretrained*\n",
    "\n",
    "Dado que en el conjunto de datos se encuentran títulos en español y portugués,\n",
    "se utilizaron [word embeddings preentrados con\n",
    "FastText](https://fasttext.cc/docs/en/pretrained-vectors.html) de estos idiomas.\n",
    "\n",
    "Para cargar el conjunto de datos se implementó la función `load_embedding` que\n",
    "requiere el vocabulario a utilizar, la dimensión de los *embeddings* descargados\n",
    "y la ruta del archivo local al ordenador en el que se está ejecutando esta\n",
    "notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61547e8",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "def load_embedding(filename, vocab, embedding_dim):\n",
    "\n",
    "    vocab_size = len(vocab) + 1\n",
    "    nof_hits = 0\n",
    "    nof_misses = 0\n",
    "\n",
    "    embedding_indexes = {}\n",
    "    with open(filename) as f:\n",
    "        _, _ = map(int, f.readline().split())\n",
    "        for line in f:\n",
    "            word, coef = line.rstrip().split(' ')\n",
    "            embedding_indexes[word] = map(float, coef)\n",
    "\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    for word in vocab:\n",
    "        vector = embedding_indexes.get(word)\n",
    "        if vector is not None:\n",
    "            embedding_matrix[word] = vector\n",
    "            nof_hits += 1\n",
    "        else:\n",
    "            nof_misses += 1\n",
    "\n",
    "    embedding_layer = Embedding(\n",
    "        vocab_size,\n",
    "        embedding_dim,\n",
    "        embeddings_initializer=tf.keras.initializers.Constant(\n",
    "            embedding_matrix),\n",
    "        trainable=False,\n",
    "    )\n",
    "\n",
    "    return embedding_layer, nof_hits, nof_misses\n",
    "\n",
    "\n",
    "filename = \"filename\"\n",
    "vocab = word_tokenizer.word_index.keys()\n",
    "embedding_dim = 300\n",
    "embedding_layer, nof_hits, nof_misses = load_embedding(filename, vocab, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878fd17e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "embedding_layer.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff25625",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Conclusión\n",
    "\n",
    "La elección de un adecuado preprocesamiento y codificación pueden ser relevantes\n",
    "durante la ejecución de un modelo. Debido a ello, se planteó una posible\n",
    "representación pero sin dejar de lado otras posibilidades. En particular,\n",
    "limpieza de texto sin eliminar *stopwords*, entrenamiento de *word embeddings*\n",
    "utilizando *FastText*, o codificación de títulos por *one-hot encoding*. A su\n",
    "vez, dado que se tienen títulos de dos idiomas distintos en el conjunto de\n",
    "datos, si se utilizan *word embeddings* disponibles online, deben traerse de\n",
    "estas dos fuentes y combinarse para formar una única matriz de vectores densos.\n",
    "En este caso, la implementación actual solo permite extraer un *word embedding*\n",
    "de una única fuente.\n",
    "\n",
    "Otro de los aspectos que es importante mejorar es la disposición de los\n",
    "*scripts* de `Python` y la jerarquía de directorios. Separarlos en directorios\n",
    "`models` y `preprocess` separado del directorio `notebooks` pero que tengan un\n",
    "solo punto de acceso es otro de los trabajos a futuro.\n",
    "\n",
    "Por último, resultaría interesante obtener una comparación entre *word\n",
    "embeddings* ya pre-entrenados junto a otros personalizados de acuerdo al\n",
    "conjunto de datos de Mercado Libre y si se observa alguna diferencia en los\n",
    "modelos de categorización."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_markers": "\"\"\"",
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
