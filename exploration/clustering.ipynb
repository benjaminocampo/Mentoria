{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7e8d54",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Categorización de publicaciones de productos de Mercado Libre\n",
    "# Autores: Maximiliano Tejerina, Eduardo Barseghian, Benjamín Ocampo\n",
    "# %% [markdown]\n",
    "# Otra método de exploración de caracteristicas es mediante técnicas de\n",
    "# aprendizaje no supervisado siendo útil para comprender aún mejor el conjunto\n",
    "# de datos con el que se dispone y mejorar la etapa de clasificación con esta\n",
    "# información.\n",
    "#\n",
    "# En particular, en esta notebook se trabajó sobre técnicas de *clustering* a\n",
    "# partir de la codificación de los títulos obtenida por los embeddings.\n",
    "# %% [markdown]\n",
    "# ## Imports y funciones *helper* necesarias\n",
    "# Nuevamente se hizo uso del conjunto de datos remoto que se trabajó en\n",
    "# distintas secciones de este repositorio. En particular aquel que tuvo los\n",
    "# títulos ya preprocesados. En caso de querer conocer más como se realizó esta\n",
    "# etapa se explicaron a detalle en el directorio `encoding/`.\n",
    "#\n",
    "# A su vez, se utilizó una muestra de 20000 ejemplares del conjunto de datos\n",
    "# para facilitar el desarrollo y la explicación de este trabajo.\n",
    "# %%\n",
    "import pandas as pd\n",
    "import fasttext\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from adjustText import adjust_text\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from yellowbrick.cluster import SilhouetteVisualizer, KElbowVisualizer\n",
    "# %%\n",
    "URL = \"https://www.famaf.unc.edu.ar/~nocampo043/ML_2019_challenge_dataset_preprocessed.csv\"\n",
    "NOF_SAMPLES = 20000\n",
    "SEED = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f264f730",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def unsupervised_data_gen(sentences, corpus_file):\n",
    "    with open(corpus_file, \"w\") as out:\n",
    "        for s in sentences:\n",
    "            out.write(s + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c91c71",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_word_embedding(vocab, vectorize):\n",
    "    return (\n",
    "        pd.DataFrame({word: vectorize(word) for word in vocab})\n",
    "        .transpose()\n",
    "        .reset_index()\n",
    "        .rename(columns={\"index\": \"word\"})\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b99eec",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_word_embedding_2DTSNE(vocab, model):\n",
    "    embedding = get_word_embedding(vocab, model)\n",
    "    X = embedding.drop(columns=[\"word\"])\n",
    "    X_TSNE = TSNE(n_components=2).fit_transform(X)\n",
    "    embedding_TSNE = pd.concat(\n",
    "        [pd.DataFrame(vocab, columns=[\"word\"]),\n",
    "         pd.DataFrame(X_TSNE)], axis=1)\n",
    "    return embedding_TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e98ec2",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_sentence_embedding(sentences, vectorize):\n",
    "    return (\n",
    "        pd.DataFrame({s: vectorize(s) for s in sentences})\n",
    "        .transpose()\n",
    "        .reset_index()\n",
    "        .rename(columns={\"index\": \"sentence\"})\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a08408d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_embedding_2DTSNE(sentences, vectorize):\n",
    "    embedding = get_sentence_embedding(sentences, vectorize)\n",
    "    X = embedding.drop(columns=[\"sentence\"])\n",
    "    X_TSNE = TSNE(n_components=2).fit_transform(X)\n",
    "    embedding_TSNE = pd.concat(\n",
    "        [pd.DataFrame(sentences, columns=[\"sentence\"]),\n",
    "         pd.DataFrame(X_TSNE)], axis=1)\n",
    "    return embedding_TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b4892d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2DTSNE(X_TSNE,\n",
    "                expressions,\n",
    "                ax,\n",
    "                point_size=50,\n",
    "                legend_size=20,\n",
    "                tick_size=20,\n",
    "                annotation_size=20,\n",
    "                annotate=True):\n",
    "    nof_words = len(X_TSNE)\n",
    "    ax.scatter(X_TSNE[:, 0],\n",
    "               X_TSNE[:, 1],\n",
    "               s=point_size)\n",
    "    ax.set_title(\"t-SNE Plot\", fontsize=legend_size)\n",
    "    annot_list = []\n",
    "\n",
    "    if annotate:\n",
    "        nof_words_to_annotate = 20\n",
    "        for i in np.random.randint(nof_words, size=nof_words_to_annotate):\n",
    "            a = ax.annotate(expressions[i], (X_TSNE[i, 0], X_TSNE[i, 1]),\n",
    "                            size=annotation_size)\n",
    "            annot_list.append(a)\n",
    "        adjust_text(annot_list)\n",
    "\n",
    "    ax.tick_params(axis='both', which='major', labelsize=tick_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca46914d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2D_kmeans(X_TSNE,\n",
    "                   km_model,\n",
    "                   ax,\n",
    "                   marker_size=2,\n",
    "                   legend_size=20,\n",
    "                   tick_size=20):\n",
    "    cluster_df = pd.DataFrame(X_TSNE)\n",
    "    cluster_df = cluster_df.assign(label=km_model.labels_)\n",
    "    sns.scatterplot(data=cluster_df, x=0, y=1, hue=\"label\", palette=\"tab20\", ax=ax)\n",
    "    ax.legend(bbox_to_anchor=(1.02, 1.02),\n",
    "              loc='upper left',\n",
    "              markerscale=marker_size,\n",
    "              prop={\"size\": legend_size})\n",
    "    ax.set_title(\"KMeans Plot\", fontsize=legend_size)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=tick_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea0c724",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def plot_silhouette(X, km_model, ax, title_size=20, tick_size=20):\n",
    "    visualizer = SilhouetteVisualizer(km_model, colors='tab20', ax=ax)\n",
    "    visualizer.fit(X)\n",
    "    ax.set_title(\"Silhoutte Plot\", fontsize=title_size)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=tick_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05329f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_elbow(X, estimator, metric, k_range, ax, title_size=10, tick_size=10):\n",
    "    visualizer = KElbowVisualizer(estimator(),\n",
    "                                  k=k_range,\n",
    "                                  metric=metric,\n",
    "                                  timings=False,\n",
    "                                  ax=ax)\n",
    "    visualizer.fit(X)\n",
    "    ax.set_title(\"Elbow Plot\", fontsize=title_size)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=tick_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec03a077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "df = pd.read_csv(URL)\n",
    "df = df.sample(n=NOF_SAMPLES, random_state=SEED)\n",
    "df = df.drop_duplicates(subset=\"cleaned_title\")\n",
    "# %% [markdown]\n",
    "# Inicialmente se proyectó la columna `cleaned_title` con los títulos de las\n",
    "# publicaciones ya preprocesadas siendo esta el corpus que se trabajó.\n",
    "#\n",
    "# Algo a recalcar es que luego del preprocesamiento se dió el caso de tener\n",
    "# algunas oraciones repetidas. Para evitar tener dos representaciones distintas\n",
    "# de la misma oración se optó por eliminar los duplicados.\n",
    "# %%\n",
    "sentences = df[\"cleaned_title\"].drop_duplicates().tolist()\n",
    "corpus_file = \"titles.txt\"\n",
    "# %% [markdown]\n",
    "# ## FastText\n",
    "# A diferencia del trabajo efectuado de clasificación y aprendizaje supervisado,\n",
    "# se utilizó FastText como algoritmo para obtener un modelo del lenguaje de los\n",
    "# títulos por medio de la API de Facebook. Se pudo haber utilizado `Gensim` pero\n",
    "# solo nos permite realizar codificación a nivel de palabra. Dado que el\n",
    "# objetivo de este trabajo es aplicar técnicas de *clustering* sobre oraciones,\n",
    "# la codificación de estas debía implementarse para esta librería. No fue este\n",
    "# el caso durante el modelo usado en clasificación ya que Keras se encargó de\n",
    "# obtener la representación vectorial de los títulos al incluir el modelo de\n",
    "# lenguaje de `Gensim` como una *embedding layer*.\n",
    "#\n",
    "# FastText para obtener la representación a nivel de palabra genera una tarea de\n",
    "# pretexto a partir de las oraciones que se le dispone por medio de un archivo,\n",
    "# en este caso `corpus_file`. Con ello se obtiene el modelo de lenguaje que se\n",
    "# utilizó para realizar esta exploración.\n",
    "# %%\n",
    "unsupervised_data_gen(sentences, corpus_file)\n",
    "# %%\n",
    "model = fasttext.train_unsupervised(corpus_file,\n",
    "                                    model=\"cbow\",\n",
    "                                    lr=0.3,\n",
    "                                    epoch=100,\n",
    "                                    dim=100,\n",
    "                                    wordNgrams=4,\n",
    "                                    ws=4)\n",
    "# %% [markdown]\n",
    "# Cada palabra en el vocabulario tiene una representación vectorial de dimensión\n",
    "# `dim=100`.\n",
    "# %%\n",
    "model.get_dimension()\n",
    "# %%\n",
    "model.get_words()[:30]\n",
    "# %% [markdown]\n",
    "# También como mostramos en otras notebooks de embeddings, la representación\n",
    "# vectorial de las palabras `barbero` y `cafetera` está a una distancia cercana\n",
    "# de otras con un significado similar.\n",
    "# %%\n",
    "model.get_nearest_neighbors(\"barbero\")\n",
    "# %%\n",
    "model.get_nearest_neighbors(\"cafetera\")\n",
    "# %% [markdown]\n",
    "# ## Clustering a nivel de palabra\n",
    "# A pesar de este no ser el objetivo se intentó mostrar como se disponen las\n",
    "# palabras en el espacio por medio de TSNE para proyectar los embeddings a 2D.\n",
    "# Sin embargo, se aplicó `KMeans` en el espacio de 100 dimensiones.\n",
    "# %%\n",
    "vocab = model.get_words()\n",
    "embedding = get_word_embedding(vocab, model.get_word_vector)\n",
    "embedding_TSNE = get_word_embedding_2DTSNE(vocab, model.get_word_vector)\n",
    "X = embedding.drop(columns=[\"word\"]).to_numpy()\n",
    "X_TSNE = embedding_TSNE.drop(columns=[\"word\"]).to_numpy()\n",
    "# %%\n",
    "kmeans = KMeans(n_clusters=5)\n",
    "kmeans.fit(X)\n",
    "# %%\n",
    "_, (ax_kmeans, ax_tsne) = plt.subplots(1, 2, figsize=(30, 10))\n",
    "plot_2DTSNE(X_TSNE, vocab, ax_tsne)\n",
    "plot_2D_kmeans(X_TSNE, kmeans, ax_kmeans)\n",
    "ax_kmeans.grid()\n",
    "ax_tsne.grid()\n",
    "plt.show()\n",
    "# %%\n",
    "_, (ax_silhouette, ax_kmeans) = plt.subplots(1, 2, figsize=(30, 10))\n",
    "plot_silhouette(X, kmeans, ax_silhouette)\n",
    "plot_2D_kmeans(X_TSNE, kmeans, ax_kmeans)\n",
    "ax_silhouette.grid()\n",
    "ax_kmeans.grid()\n",
    "plt.show()\n",
    "# %% [markdown]\n",
    "# Sin embargo a pesar de los gráficos, no se alcanza a percibir agrupamientos a\n",
    "# través de la representación con TSNE. Sin embargo, la cercania de los\n",
    "# significados se mantiene. Se cree que la disposición de las palabras en el\n",
    "# `blob` si ocurren. Por ejemplo que las palabras más relacionadas con\n",
    "# `cafetera` se encuentran en el hemisferio izquierdo del `blob`.\n",
    "# %% [markdown]\n",
    "# ## Clustering a nivel de títulos\n",
    "# Para esta sección se obtuvo el embedding de una oración tomando las\n",
    "# representaciones individuales de cada palabra que la componen, dividiendo esos\n",
    "# vectores por su norma y considerando el promedio de ellas. Esto realiza\n",
    "# internamente FastText por medio de `get_sentence_vector`.\n",
    "# %%\n",
    "embedding = get_sentence_embedding(sentences, model.get_sentence_vector)\n",
    "embedding_TSNE = get_sentence_embedding_2DTSNE(sentences,\n",
    "                                               model.get_sentence_vector)\n",
    "X = embedding.drop(columns=[\"sentence\"]).to_numpy()\n",
    "X_TSNE = embedding_TSNE.drop(columns=[\"sentence\"]).to_numpy()\n",
    "# %%\n",
    "embedding\n",
    "# %%\n",
    "embedding_TSNE\n",
    "# %% [markdown]\n",
    "# ## Elbow Method\n",
    "# Con el fin de seleccionar la cantidad óptima de clusters, una de las\n",
    "# estrategias usadas fue el `Elbow Method` que consiste en seleccionar un rango\n",
    "# de valores `k_range` ajustando el modelo de `KMeans`. Con ello confeccionamos\n",
    "# un gráfico de linea de tal manera que, si este se asemeja al de un codo,\n",
    "# entonces el punto de inflección de la curva es un buen indicador de cual es el\n",
    "# mejor modelo. La métrica utilizada en este caso fue `distortion` que consiste\n",
    "# en la suma de las distancias al cuadrado de cada punto al centro de los\n",
    "# clusters.\n",
    "# %%\n",
    "fig, ax_elbow = plt.subplots()\n",
    "k_range = (2, 20)\n",
    "plot_elbow(X, KMeans, \"distortion\", k_range, ax_elbow)\n",
    "plt.show()\n",
    "# %% [markdown]\n",
    "# Si bien no se encuentra ningun codo pronunciado, el algoritmo encuentra que la\n",
    "# cantidad óptima es de 11 clusters.\n",
    "# %%\n",
    "kmeans = KMeans(n_clusters=11)\n",
    "kmeans.fit(X)\n",
    "# %% [markdown]\n",
    "# ## Silhouette\n",
    "# Una vez ajustado para la cantidad de clusters dada con el método anterior se\n",
    "# utilizó el método de silhouette junto a una visualización de los clusters\n",
    "# ajustada en las 100 dimensiones aproximada en 2D usando TSNE.\n",
    "# %%\n",
    "_, (ax_silhouette, ax_kmeans) = plt.subplots(1, 2, figsize=(30, 10))\n",
    "plot_silhouette(X, kmeans, ax_silhouette)\n",
    "plot_2D_kmeans(X_TSNE, kmeans, ax_kmeans)\n",
    "ax_silhouette.grid()\n",
    "ax_kmeans.grid()\n",
    "plt.show()\n",
    "# %% [markdown]\n",
    "# Notar que el coeficiente de silhouette promedio de cada uno de los clusters es\n",
    "# de aproximadamente 0.1.\n",
    "# %% [markdown]\n",
    "# ## Top 10 categorías más presentes en cada cluster\n",
    "# %%\n",
    "df = df.assign(cluster=kmeans.labels_)\n",
    "# %%\n",
    "df_cross = pd.crosstab(df[\"category\"], df[\"cluster\"])\n",
    "df_cross\n",
    "# %%\n",
    "for cluster in df_cross:\n",
    "    print(df_cross[cluster].sort_values(ascending=False).head(10))\n",
    "# %% [markdown]\n",
    "# Notar que para cada uno de los tops hay categorias que tienen una pequeña\n",
    "# cantidad de ejemplares. Por lo tanto solo consideraremos aquellas que superen\n",
    "# los 100.\n",
    "# %%\n",
    "for cluster in df_cross:\n",
    "    print(\n",
    "        df_cross\n",
    "        .loc[df_cross[cluster] > 100, cluster]\n",
    "        .sort_values(ascending=False)\n",
    "    )\n",
    "    print()\n",
    "# %% [markdown]\n",
    "# Solo algunos cluster se puede comentar que podrían llegar a tener alguna\n",
    "# relación en cuanto a su significado. Tales son el caso de los clusters:\n",
    "#  0: Relacionado a elementos de cocina.\n",
    "#  2: Productos para bebé.\n",
    "#  5: Ropa.\n",
    "#  7 y 9: Nuevamente elementos de cocina y ropa.\n",
    "# %% [markdown]\n",
    "# ## Porcentaje de títulos a cada cluster\n",
    "# De manera similar podemos obtener, para cada categoría, el porcentaje de\n",
    "# títulos que pertence a cada cluster. Obteniendo una representación similar a\n",
    "# la anterior\n",
    "# %%\n",
    "df_cross_percent = pd.crosstab(df[\"category\"], df[\"cluster\"], normalize=\"index\")\n",
    "df_cross_percent\n",
    "# %%\n",
    "for cluster in df_cross_percent:\n",
    "    print(df_cross_percent[cluster].sort_values(ascending=False).head(10))\n",
    "# %% [markdown]\n",
    "# Nuevamente podemos optar por considerar solo aquellas categorías con un\n",
    "# porcentaje superior a un umbral, en este caso a 0.1.\n",
    "# %%\n",
    "for cluster in df_cross_percent:\n",
    "    print(\n",
    "        df_cross_percent\n",
    "        .loc[df_cross_percent[cluster] > 0.1, cluster]\n",
    "        .sort_values(ascending=False)\n",
    "    )\n",
    "    print()\n",
    "# %% [markdown]\n",
    "# ## Conclusiones\n",
    "# Algunos desafios con los que nos encontramos es la posibilidad de utilizar el\n",
    "# conjunto completo de datos para realizar los experimentos. Una posibilidad\n",
    "# para mejorar el modelo de lenguaje obtenido sería utilizar todos los\n",
    "# ejemplares en lugar de un sampleo.\n",
    "#\n",
    "# Los resultados obtenidos con los embeddings de FastText para los títulos\n",
    "# tampoco fueron muy significativos para obtener alguna conclusión sólida de los\n",
    "# agrupamientos. Una posiblidad podría ser probar distintos parámetros del\n",
    "# modelo de lenguaje o utilizar otros métodos de vectorización (BOW, matriz\n",
    "# de co-ocurrencia, triplas de dependencia, etc) de tal manera de poder\n",
    "# compararlos."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('datasc': conda)",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
